{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4511b7",
   "metadata": {},
   "source": [
    "# 25-26: 4369 -- PROGRAMMING FOR DATA ANALYTICS\n",
    "## Big Project\n",
    "\n",
    "<img src=\"../images/Programming.png\" alt=\"progrsmming\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f8320d",
   "metadata": {},
   "source": [
    "One of the main challenges of this project (I found) was deciding on a suitable data subject to research and then attempting to **source** data.  \n",
    "While I looked into data for Retail Sales, Traffic Accidents, Weather, etc (and browsing Kaggle for potential subjects) ... after several failed attempts, I found that *I needed a pursue a subject I was genuinely curious about.*  \n",
    "\n",
    "I decided to look into the subject of **Trends in the use of Programming Languages** and to leverage the huge data repository of **GitHub** as a data source.  \n",
    "  \n",
    "I wanted to potentially look at the following Programming Language Trends\n",
    "* Analyze which languages are growing/declining over time\n",
    "* Analyze language switching patterns (Devs moving from language x to language y)\n",
    "* Has the recent explosion of AI resulted in a rise of AI-related repositories \n",
    "    * How have these AI-related repositories grown in recent years\n",
    "    * How does this growth compare with traditional languages\n",
    "* Compare developer activity by geography and timezone\n",
    "    * How does India and China compare with EU/US in github activity \n",
    "* Visualize the shift from older to newer languages\n",
    "\n",
    "\n",
    "GitHub Data Sources are available in :  \n",
    "* GitHub API - Free tier allows decent access  \n",
    "* GH Archive - Historical GitHub event data (billions of events!)  \n",
    "* GitHub's public datasets on Google BigQuery  \n",
    "* Pre-made datasets on Kaggle  \n",
    "\n",
    "I initially looked into the GitHub API as a mechanism to extract the datasets I would require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b3026e",
   "metadata": {},
   "source": [
    "***\n",
    "## The GitHub REST API with Python\n",
    "### (... and its limitations üòñ)\n",
    "\n",
    "I experimented a while with Python scripts that collect GitHub Metadata and save to CSV files.  \n",
    "https://blog.apify.com/python-github-api/  \n",
    "https://melaniesoek0120.medium.com/how-to-use-github-api-to-extract-data-with-python-bdc61106a501  \n",
    "\n",
    "I also investigated the use of the PyGithub Library.   \n",
    "https://github.com/PyGithub/PyGithub  \n",
    "https://pygithub.readthedocs.io/en/latest/introduction.html  \n",
    "https://stackoverflow.com/questions/10625190/most-suitable-python-library-for-github-api-v3  \n",
    "https://www.youtube.com/watch?v=QaURSdmP0o8  \n",
    "\n",
    "But, this only highlighted how restrictive this API's \"Rate Limit\" can be.  \n",
    "Look at script below (generated with help of Claude AI) that uses pyGithub module to attempt to look at the count of language specific  repositories created with a specific period  \n",
    "\n",
    "[My Python Script for Extracting Github Data](./testing/github_api_test.py)  \n",
    "Note: To use the PyGithub Module you will need to install PyGithub library with  \n",
    "`pip install PyGithub`  \n",
    "\n",
    "If you look at the results of this script (output to [this text file](./testing/Github_REST_API_Results.txt)), all language for all months return exactly same result ... \"1,000 repos\"  \n",
    "**I am obviously hitting a limit of some sort here.**\n",
    "This is GitHub Search API‚Äôs ‚Äú1,000-result cap‚Äù\n",
    "   \n",
    "A simplified version (with a hard-coded sample) is shown below where even a reduced time interval of only 3 hours is used...   \n",
    "It still returns a count close to the limit of 1000 repositories   \n",
    "\n",
    "*Experimentation with values for language, start_date, and end_date highlight how restrictive this 1000 limit is.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555e1408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: ngn73\n",
      "Count of Python repos created between 2024-03-01T00:00:00+00:00 and 2024-03-01T03:00:00+00:00: 940\n"
     ]
    }
   ],
   "source": [
    "from github import Github, RateLimitExceededException, Auth\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the token from environment variables\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "\n",
    "# Create authentication object\n",
    "auth = Auth.Token(GITHUB_TOKEN)\n",
    "# Initialize GitHub client with auth\n",
    "g = Github(auth=auth)\n",
    "\n",
    "# Verify authentication\n",
    "try:\n",
    "    user = g.get_user()\n",
    "    print(f\"Authenticated as: {user.login}\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication failed: {e}\")\n",
    "\n",
    "# Example usage \n",
    "language = 'Python'\n",
    "start_date = '2024-03-01T00:00:00+00:00'\n",
    "end_date = '2024-03-01T03:00:00+00:00'\n",
    "\n",
    "# Build the API search query\n",
    "query = f\"language:{language} created:{start_date}..{end_date}\"\n",
    "# Search repositories with PyGithub\n",
    "result = g.search_repositories(query=query)\n",
    "# Get total count\n",
    "total_count = result.totalCount\n",
    "print(f\"Count of {language} repos created between {start_date} and {end_date}: {total_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa94ae",
   "metadata": {},
   "source": [
    "<font color=\"crimson\">... **I needed to find an alternative (less restrictive) method to extracting Github Repository metadata.**</font>   \n",
    "\n",
    "<font color=\"yellow\">But, there are still some advantages to using the GitHub API where this limit is not applicable to extracting geo-based data (more on that later)</font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483bc969",
   "metadata": {},
   "source": [
    "***\n",
    "## Google BigQuery + GitHub Archive  \n",
    "  \n",
    "As a potential alternative to using the GitHub REST API (with Python), Google's Cloud-based Data Warehouse Service \"BigQuery\" provides a series of tools for extracting and analyzing large scale databases (sometimes billions of rows)   \n",
    "BigQuery abstracts the complexity of underlying server/database infrastructure. You just load data and run standard SQL queries against it.  \n",
    "https://console.cloud.google.com/bigquery/  \n",
    "https://www.datacamp.com/tutorial/beginners-guide-to-bigquery  \n",
    "https://www.udemy.com/course/introduction-to-google-cloud-bigquery/  <font color=\"crimson\">*(requires subscription)</font>\n",
    "\n",
    "More importantly, Google is also in collaboration with GitHub providing access to an incredible new Github Archive database via **Google BigQuery**  \n",
    "The Google BigQuery Public Datasets program now offers a full snapshot of the content of more than 2.8 million open source GitHub repositories in BigQuery  \n",
    "This provides a alternative mechanism to analyze the source code of almost 2 billion files with a simple (or complex) SQL query without the limitations experienced above.   \n",
    "  \n",
    "\n",
    "https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-source-code  \n",
    "https://hoffa.medium.com/github-on-bigquery-analyze-all-the-code-b3576fd2b150  \n",
    "https://hoffa.medium.com/400-000-github-repositories-1-billion-files-14-terabytes-of-code-spaces-or-tabs-7cfe0b5dd7fd  \n",
    "https://github.com/fhoffa/analyzing_github?tab=readme-ov-file  \n",
    "\n",
    "The use of SQL is ideal as I am quite comfortable with the SQL syntax  \n",
    "https://docs.cloud.google.com/bigquery/docs/reference/rest?apix=true   \n",
    "https://codelabs.developers.google.com/codelabs/bigquery-github#0  \n",
    "\n",
    "There is a lot of menus within Google Cloud. Within Google Cloud, goto Navigation Menu -> BigQuery->Studio and select 'SQL Query'\n",
    "<img src=\"../images/Goole_BigQuery_SQL.png\" alt=\"Google BigQuery\" width=\"600\">  \n",
    "\n",
    "**SQL queries within BigQuery can be used to structure, clean, and prepare the data so it meets the needs of my analysis   \n",
    "Results can then be saved as CSV or JSon files for further processing/visualizing with Python.**  \n",
    "  \n",
    "The core tables to query against fall under 2 categories:  \n",
    "1. \"githubarchive\" Time based activity Tables (examples below)\n",
    "* **githubarchive.day.20241201** (Archive of events for day December 1, 2024)  \n",
    "* **githubarchive.month.202411** (Archive of events for Month November 2024)  \n",
    "* **githubarchive.year.2024** (Archive of events for Year 2024)  \n",
    "\n",
    "To query the whole of 2024 you can also use a wildcard in the Table name (e.g. githubarchive.day.2024*)   \n",
    "\n",
    "2. \"github_repos\" Snapshots of GitHub repository contents, metadata, commits, etc.\n",
    "* **commits**\n",
    "*  **files**\n",
    "*  **contents**\n",
    "*  **languages**\n",
    "   \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "The details of each Gitub event (push, pull, create, frk, etc.) are stored within the \"payload\" field. The content of the \"payload\" field is different for each event type and may be updated by GitHub at any point, hence it is kept as a serialized JSON string value in BigQuery. JSON_EXTRACT functions can be used to apply filters to this Field  \n",
    "  \n",
    "https://www.gharchive.org/#:~:text=The%20content%20of%20the%20%22payload,access%20data%20in%20this%20field.  \n",
    "https://github.com/igrigorik/gharchive.org/issues/148  \n",
    "  \n",
    "  \n",
    "An example SQL script below filters 'pull' events for Python and Javascript languages using **JSON_EXTRACT_SCALAR( )**\n",
    "\n",
    "```\n",
    "SELECT \n",
    "    FORMAT_DATE('%Y-%m', PARSE_DATE('%y%m%d', _TABLE_SUFFIX)) as month,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.base.repo.language') as language,\n",
    "    COUNT(*) as pr_count,\n",
    "    COUNT(DISTINCT repo.name) as active_repos\n",
    "FROM `githubarchive.day.20*`\n",
    "WHERE \n",
    "    type = 'PullRequestEvent'\n",
    "    AND JSON_EXTRACT_SCALAR(payload, '$.pull_request.base.repo.language') IN ('Python', 'JavaScript')\n",
    "    AND _TABLE_SUFFIX BETWEEN \n",
    "        FORMAT_DATE('%y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH))\n",
    "        AND FORMAT_DATE('%y%m%d', CURRENT_DATE())\n",
    "GROUP BY month, language\n",
    "ORDER BY month DESC, pr_count DESC;\n",
    "```\n",
    "\n",
    "\n",
    "It could also be argued that the full data analysis could be performed completely within the BigQuery Studio as it offers a wide range of advanced tools for Analysis.  \n",
    "But, in the interest of displaying the use of a range of tools covered in Module *25-26: 4369 -- Programming for Data Analytics* with an emphasis on Python, I will export datasets from BigQuery as CSV/JSon and later process with Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bfdd0",
   "metadata": {},
   "source": [
    "***\n",
    "## Using BigQuery and Python\n",
    "Python can integrate with BigQuery Datasources with the **google-cloud-bigquery** package.  \n",
    "https://docs.cloud.google.com/python/docs/reference/bigquery/latest  \n",
    "This will require the Setup of authentication that will require additional configuration (application credentials, project_id, etc.).  \n",
    "But, in the interests of migrating code other machines (and allowing code to be potentially tested by Andrew), I would definitely be better off using the BigQuery console directly for simpler, manual exports of SQL Data Resultsets (into CSV or JSon format)  \n",
    "  \n",
    "**So I will use the BigQuery console directly to generate CSV or JSon result Files that will act as the Data-source for Analysis Project.**\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
